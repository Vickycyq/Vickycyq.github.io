[{"content":"1. AIGC简介 1.1 概念 1.2 大语言模型简介 语言模型的本质是对任意一段文本序列的概率进行建模\n如果将语言模型看成一个大黑盒的话，它的输入是一段文本序列，输出也是一段文本序列，通过训练语言模型，就能使得给定的文本序列和输出的文本序列拼接起来所组成的一整段文本序列的概率尽可能比较大\n为了得到更好的泛化模型，以及提高模型在执行多种通用任务上的能力，基于大数据的预训练模型越来越多。 给定更大的预训练数据，Transformer 架构在增加模型大小和训练中计算量的情况下表现更好，展示了卓越的 scalability。具体来说，基于 transformer 的语言模型的性能与模型参数、数据量和训练计算量的呈幂律关系（Kaplan 等人，2020 年）。transformer 在语言模型上的架构有三种不同的设计模式：encoder-only(Bert)\\encoder-decoder(Bart\\T5)\\decoder-only(GPT)。\n大规模语言模型所采用的都是 decoder-only 架构，并采用世界上丰富的未标签文本进行自监督训练。预训练任务通常是Next word prediction，这种方式又被称为 Causal language modeling。这个Causal就是“因果”的意思，对于decoder，它在训练时是无法看到全文的，只能看到前面的信息。因此这类模型适合做文本生成任务.使用decoder transformer的 GPT 预训练，其中目标序列是移动了一个标记的输入序列。请注意，transformer 解码器中的因果注意力模式强制每个 token 只能关注其过去的 token。GPT 有 1 亿个参数，需要针对下游任务进行微调。一年后引入的更大的语言模型 GPT-2（Radford 等人，2019 年）。与 GPT 中的原始 transformer decoder 相比，GPT-2 中采用了pre-normalization以及改进的初始化和权重缩放。在 40 GB 的文本上进行预训练，15 亿参数的 GPT-2 在语言建模基准上获得了 sota，并在不需要更新参数和架构的情况下在多个其他任务上获得了可观的结果。\n少样本与零样本的唯一区别就是中间多出了一些参考样例，它们其实都是在续写前缀（只是零样本的输入没有任何参考，而少样本的输入有一些参考样例来帮助语言模型推断如何根据任务输入生成相应的任务输出\n2. 提示词工程简介 2.1 基本概念 2.2 提示词要素及设计通用技巧 2.3 提示词模板的重要性 Zhao et al.(2021)的论文证明了in-context learning的表现比zero-shot好，但是比较少有work探索demonstration是如何帮助模型提升performance的，因此Min et al.(2022)通过实验分析探索这个问题。\nIn-context learning（ICL）是指上下文学习，通过用自然语言模版编写一些示例（case）并为case打上标签，将其输入到语言模型中对input进行预测。 下面这张图片可以解释demonstration是什么： 作者实验了12个模型，其中有6个语言模型，语言模型的各种配置如下所示： 作者对四个潜在的learning signal进行探究： （1）The input-label mapping：即每个input xi是否对应正确的label yi （2）The distribution of the input text，输入的K个文本x1\u0026hellip;xk在不同数据集的分布（input是分布在同一个数据集还是来自于不同于当前任务的数据集） （3）The label space，即y1\u0026hellip;yk这K个标签是在标签空间中选择还是用随机英文单词作为标签 （4）The Format，即格式/模板，如input-label格式 作者得出以下结论：\n（1）Gold labels 和 random labels 从上图结果可以看出， 使用demostration模型performance比不使用好 gold labels表示正确的labels，random labels表示正确的标签被替换为随机标签；random label对模型性能的破坏并不是很大。说明in-context learning更多的是去学习task-specific的format，通过模型以往学习的内容根据提供的format输出内容，而不是通过input-output mapping输出内容 从上图结果可以看出， 标签的准确率对模型的表现没有很大影响，这进一步说明in-context learning确实是学习task-specific的format，而不是input-output mapping 用错误的标签的demonstration模型的performance也比不用demonstration好 （2）The distribution of the input text 观察红色（Input都是同一个数据集分布的）和紫色（Input来自不同于当前任务的数据集分布）两个图，可以发现大多数任务上差异很大。说明选择不同分布的Input对ICL的性能影响很大。 （3）The label space Glod labels比Random Labels高，但是差异非常小，说明标签是否来自于同一个分布并不重要；但是另一篇论文认为，Input-Output Mapping是很重要的，因此Label的正确与否也会对一些任务存在影响，只是在一些简单的任务上体现不出，这取决于任务的类型，当然也取决于模型本身。 （4）The Format 去掉format后，发现与no demonstrate相比没有明显的提升，说明format是很重要的（即label和input text 缺一不可） 结论：\n在in-context learning中，模型不是在所给的demonstration中做input-ouput mapping而生成结果，而是通过demonstration唤醒模型以往所学知识从而生成结果； demonstration的format很重要，有无demonstration和有无format对模型的performance影响都大 ","permalink":"https://vickycyq.github.io/zh/posts/blog/prompt-engineering/%E7%AE%80%E4%BB%8B/prompt/","summary":"1. AIGC简介 1.1 概念 1.2 大语言模型简介 语言模型的本质是对任意一段文本序列的概率进行建模 如果将语言模型看成一个大黑盒的话，它的输入是一段文本序列","title":"AIGC及提示词工程简介"},{"content":"\u0026ldquo;Chain-of-Thought Prompting\u0026quot;是指在demonstration中设计一系列连贯的思维步骤从而引导模型在面对复杂的推理任务时，将问题分解成多个中间步骤，并逐一解决，然后给出最后的答案。\n在CoT提出之前，关于大规模语言模型的推理任务中，有两种方法，一是针对下游任务对模型进行微调，但是微调成本高；二是为模型提供少量输入输出样例进行学习，传统的输入输出样例在推理任务上效果比较差。针对这个问题，Wei et al. (2022)提出Chain-of-Thought这种提示技术: 上图展示了few-shot prompting和chain of thought prompting方法求解推理任务的对比，few-shot prompting在推理任务中表现并不好 而CoT在demonstration中不仅给出了答案，还人为写出了中间的推理步骤。给出了根据CoT方式写的demonstration后，语言模型会自动先根据任务写出中间推理步骤，根据步骤给出正确答案 作者还在论文中提到了以下几点：\nCoT是将一个多步骤的复杂推理任务分解出中间步骤，而语言模型在输出的时候是一个一个token进行生成的，因此问题越难，步骤越多，生成的token数量就越多，因此越难的问题使用到的计算量就越大，计算机分配的计算资源也越多 在不知道答案的情况下，用CoT能同时知道中间推理步骤 接下来让我们使用CoT来解决上一节的推理问题： 输入：\n1 2 3 4 5 6 A、B是两个长方体物块，叠放在水平地面C上，A叠放在B上，F是作用在物块B上沿水平方向右的力，大小为200N，物块A和B以相同的速度做匀速直线运动，由此可知，关于A、B间摩擦力F1为0N，B、C间摩擦力为200N。 A：由于物块A和B以相同的速度做匀速直线运动，因此A受力平衡，合力为0，A不受摩擦力，所以A与B之间的摩擦力F1为0N；物块B受水平向右的力200N，同时以相同的速度做匀速直线运动，合力为0，因此物块B受水平向左的摩擦力200N，BC间的摩擦力为200N，答案是True 水平地面上叠放A、B两个物体，A在B上面，A、B大小一样，给A施加一个向右的拉力F1=5N，给B施加一个向左的拉力F2=3N，那么物体B的上下表面所受到摩擦力的大小为5N和3N A：由于物块A和B同时以相同的速度做匀速直线运动，因此A、B的合力均为0，A受向右的拉力5N，所以AB之间有向左的摩擦力5N；B受向左的拉力3N，向右的与A之间的摩擦力5N，因此B与地面之间的摩擦力是向左的2N，答案是False 水平面上叠放着A、B两个物体，A在B上面，在水平方向力F的作用下，相对静止，一起向左做匀速直线运动，已知F=5N，物体B上下表面所受摩擦力分别是5N和5N A： ChatGPT输出：\n1 对于第三个情况，答案可能是 False。物块 B 与地面之间的摩擦力应该受到施加在 B 上的力 F=5N 的影响，但具体的摩擦力大小取决于物块 B 和地面之间的摩擦系数。如果摩擦系数足够大，以至于可以平衡施加在 B 上的力 F=5N，那么 B 与地面之间的摩擦力可能为 5N。但如果摩擦系数较小，不能完全平衡力 F，那么摩擦力可能小于 5N。所以答案会取决于具体的摩擦系数值。 从上面的回答可以看出，ChatGPT给出了正确的答案，但是还是没有完全把A和B的受力情况答对。\nZero-shot-CoT：\n上面我们写的例子是mannual-CoT,即在输入问题前，手动设计一些问题和答案的样例，同时给出中间推理步骤，因此我认为在这种方式下，设计有代表性的样例和推理步骤比较重要。 而zero-shot-CoT就是在输入问题的时候，连接“let\u0026rsquo;s think step by step\u0026quot;让语言模型自动续写推理步骤，并最终生成答案。 我们看看以下例子：\n输入：\n1 2 水平面上叠放着A、B两个物体，A在B上面，B在水平方向力F的作用下，相对静止，一起向左做匀速直线运动，已知F=5N，物体B下表面与地面之间的摩擦力5N，A与B之间的摩擦力是0N，这是正确的吗？let\u0026#39;s think step by step A： ChatGPT输出：\n1 2 3 4 5 6 物体 B 处于水平面上，所以有地面对物体 B 的支持力（垂直向上）以及重力（垂直向下）。由于物体 B 相对静止，这两个力必须平衡。 物体 B 与地面之间的支持力等于物体 B 的重力，这可以用牛顿的第三定律解释。支持力等于物体 B 对地面的作用力，也等于地面对物体 B 的反作用力，它们大小相等。 物体 B 在水平方向上受到外力 F=5N 的作用，这是一个向左的力。 由于物体 B 相对静止，水平方向上的外力 F 必须由摩擦力来平衡，以防止物体 B 滑动。 物体 A 在物体 B 上，如果 A 与 B 之间没有水平方向上的相对运动，那么它们之间的摩擦力应该是零。 综上所述，物体 B 下表面与地面之间的摩擦力是 5N，以防止它滑动，而 A 与 B 之间的摩擦力是零，因为它们之间没有相对运动。所以，这个情况是正确的。 从上面的例子可以看出，ChatGPT通过“let\u0026rsquo;s think step by step\u0026quot;这句话就能自动续写推理过程，并最终得出正确答案。在这个过程中，我曾修改了题目的描述，因此我认为清晰避免歧义的题目描述也有助于ChatGPT理解文本并按步骤推理。\nAuto-CoT：\nZhang et al.(2022)在论文中尝试使用Auto-CoT，即先把示例问题加上“let\u0026rsquo;s think step by step\u0026quot;这句话，让语言模型自动生成推理步骤，然后将示例加上生成的推理步骤以及答案拼接起来，构成few-shot样例，最后加上输入的问题让语言模型写中间推理步骤以及答案。 ","permalink":"https://vickycyq.github.io/zh/posts/blog/prompt-engineering/%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8A%80%E6%9C%AF/cot_prompting/","summary":"\u0026ldquo;Chain-of-Thought Prompting\u0026quot;是指在demonstration中设计一系列连贯的思维步骤从而引导模型在面对复杂的推理任务时，将问题分解成多","title":"Chain-of-Thought Prompting"},{"content":"在完成更复杂的任务时，零样本提示表现不佳，此时可以使用少样本提示（few-shot prompting），启用in-context learning，通过为模型提供demonstration引导模型实现更好的性能。\n“Few-Shot Learning\u0026quot;是指机器学习或深度学习中的一种训练策略，其中模型在处理训练数据时只使用非常有限数量的样本或示例。模型通过仅使用少量样本来学习任务，从而更好地泛化到未见过的数据。这对于许多应用场景非常重要，特别是在数据稀缺的情况下，例如医疗诊断、自然语言处理中的语言翻译等领域。 “Few-Shot Prompting\u0026quot;是一种用于预训练语言模型的技术，旨在将模型引导到执行特定任务的方式.“少样本”通常表示提示的信息非常有限，但是模型能根据这个提示来生成答案或执行任务. zero-shot和one-shot对比示例：\n输入：\n1 请用farduddle这个词造句 ChatGPT输出：\n1 他在寒风中等待了一个小时，完全感到了冷到了骨子里，但他坚持在那里farduddle着，直到朋友终于出现了。 可以看出，ChatGPT并没有理解这个单词，因此造出来的句子词不达意。接下来用 Brown et al.（2020）的例子做one-shot prompting。 输入：\n1 2 3 “whatpu”是坦桑尼亚的一种小型毛茸茸的动物。一个使用whatpu这个词的句子的例子是： 我们在非洲旅行时看到了这些非常可爱的whatpus。 “farduddle”是指快速跳上跳下。一个使用farduddle这个词的句子的例子是： ChatGPT输出：\n1 孩子们在操场上高兴地farduddle着，玩得不亦乐乎。 在这个例子中，我们给了ChatGPT一个demonstration，ChatGPT已经能根据给的demonstration执行任务，对于更困难的任务，可以增加demonstration的个数（5-shot, 10-shot）。\nZhao et al.(2021)的论文指出：\ndemonstration的format很重要，即使只是使用随机标签，模型的performance也比没有标签好 demonstration的input的分布对模型的in-context learning性能影响很大，而label space的分布则不重要 而另一篇论文则有不完全相同的研究结论，Min et al., (2022)的论文则指出：\nlabel space和demonstration的输入文本的分布都很重要（标签是否正确） 使用的格式也对性能起着关键作用，即使只是使用随机标签，这也比没有标签好得多 其他结果表明，从真实标签分布（而不是均匀分布）中选择随机标签也有帮助 我们给下面的外卖评论分配随机标签，看ChatGPT能否正确完成任务： 输入：\n1 2 3 4 这家分店的五分糖巨甜！！！比我喝过的其他店的都甜很多//负面 今天的要么太咸，要么太淡，牛肉还没烤干，都丢了没吃//正面 吃过很多次了，连带淄博小饼套餐，很喜欢烤牛肉和鸡翅//负面 记不清第几次点了，好吃太好吃了，简直是本干饭人心中的白月光// ChatGPT输出：\n1 最后一个评论非常正面，表达了对食物的喜爱。 可以看出，即使给评论分配错误的标签，ChatGPT还是正确完成了任务\n执行更复杂的任务时，少样本提示的作用不大 输入：\n1 2 3 4 5 6 A、B是两个长方体物块，叠放在水平地面C上，A叠放在B上，F是作用在物块B上沿水平方向右的力，大小为200N，物块A和B以相同的速度做匀速直线运动，由此可知，关于A、B间摩擦力F1为0N，B、C间摩擦力为200N。 A：答案是True 水平地面上叠放A、B两个物体，A在B上面，A、B大小一样，给A施加一个向右的拉力F1=5N，给B施加一个向左的拉力F2=3N，那么物体B的上下表面所受到摩擦力的大小为5N和3N A：答案是False 水平面上叠放着A、B两个物体，A在B上面，在水平方向力F的作用下，相对静止，一起向左做匀速直线运动，已知F=5N，物体B上下表面所受摩擦力分别是5N和5N A： ChatGPT输出：\n1 在水平面上叠放A、B两个物体，它们以相同的速度做匀速直线运动，外力F=5N。因为它们以相同的速度运动，A、B之间的摩擦力应该平衡外力F，所以它们之间的摩擦力都是5N。这个陈述是True。 显然，ChatGPT给出了错误的答案，因为完成这个任务需要涉及更多的推理步骤，所以需要更高级的prompting技术，比如将步骤写成demonstration提示AI从而完成任务，下一节将介绍Chain-of-Though Prompting.\n","permalink":"https://vickycyq.github.io/zh/posts/blog/prompt-engineering/%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8A%80%E6%9C%AF/%E5%B0%91%E6%A0%B7%E6%9C%AC%E6%8F%90%E7%A4%BA/","summary":"在完成更复杂的任务时，零样本提示表现不佳，此时可以使用少样本提示（few-shot prompting），启用in-context learni","title":"Few-Shot Prompting"},{"content":"","permalink":"https://vickycyq.github.io/zh/posts/project/flan%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/","summary":"","title":"FLAN微调实战"},{"content":"\u0026ldquo;Zero-Shot Prompting\u0026quot;也称为“零样本提示”，是指在机器学习和自然语言处理中的一种技术，用于解决模型在处理未曾见过的、未经训练的类别或者数据时的问题。传统的机器学习模型需要在训练阶段接触到每个类别的数据样本，然后才能对其进行分类或者执行其他任务。但是，通过零样本提示，模型可以利用先前学习到的知识来完成该任务。\n例如，在自然语言处理中，如果模型已经训练了大量词汇，但是在测试的时候遇到了一个新的词汇，零样本提示可以让模型通过词汇的词义、上下文或者其他信息来推测该新词汇的含义，而不需要提供新词汇的训练样本。\n以下是零样本提示的使用示例：\n1 2 3 4 将文本分类为中性、负面、正面。 文本： 就这么点，不知道的还以为我要求的。少得可怜，我自己做都比这个多，这么点东西好意思收我60。 情感： ChatGPT输出：\n1 2 根据提供的文本，可以将其情感分类为负面。 这段文本表达了对某项要求的不满和对提供的服务或产品的不满意。 在这个例子中，我没有告诉ChatGPT什么样的文本是中性、负面或者正面，但是ChatGPT能正确对文本进行分类。\nWei et al.(2021)的研究采用了一个包含1370亿个参数的预训练模型，并在60多个自然语言指令模版下描述的NLP数据集上进行了instruction tune,并将instruction-tuned model称为FLAN。从下面的图片可以看出，instruction-tuning能有效提高语言模型的zero-shot学习能力。\nInstruction-tuning是一种微调大型预训练语言模型的方法，可以让模型根据特定任务提供指令来执行任务。 Fine-tuning和instruction tuning以及Prompt-tuning的区别：\nFine-tuning 是指采用预训练的模型（通常是在大规模数据上进行预训练的模型，如BERT、GPT等）作为初始模型，然后在特定任务的小规模数据上进行进一步训练。这种方法允许将大型通用模型迁移到特定任务上，以适应特定的数据和任务需求。 Instruction tuning模型不仅接受任务数据，还接受一种被称为\u0026quot;指导\u0026quot;或\u0026quot;说明\u0026quot;的自然语言输入，以帮助模型更好地适应特定任务。 Prompt tuning中，任务描述或提示文本（prompt）被设计成模型输入的一部分,是一种用于改善任务性能的技术,通常用于问答和文本生成任务。 Fine-tuning 是一种通用的微调方法，而instruction tuning 是fine-tuning 的一种变种，它利用自然语言指导来指导模型在特定任务上的微调，从而提高模型的性能。Instruction tuning 的关键优势在于它可以更灵活地适应多样的任务和指导。 当零样本提示不起作用时，可以适当在提示中提供演示或者示例，这种提示方式称为少样本提示。下一节将介绍少样本提示。\n","permalink":"https://vickycyq.github.io/zh/posts/blog/prompt-engineering/%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8A%80%E6%9C%AF/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E6%8F%90%E7%A4%BA/","summary":"\u0026ldquo;Zero-Shot Prompting\u0026quot;也称为“零样本提示”，是指在机器学习和自然语言处理中的一种技术，用于解决模型在处理未曾见过的、未经训练的类别","title":"Zero-Shot Prompting"},{"content":"结论 AIGC可在游戏生产端、游戏运营端以及玩家体验端全方位赋能游戏产业，帮助游戏企业实现降本增效，开发创新玩法，提高玩家在游戏过程中的各项体验； 科技大厂收购游戏公司，纷纷入局游戏行业，通过自身AI技术与游戏公司行业经验结合，探索并加速AI对游戏的赋能与改造； 各游戏公司迅速布局AI并推动AI技术在游戏领域落地，市场反应良好，用户接受度高； AIGC赋能食品饮料行业人-货-场全链条条，构筑沉浸式社会营销，帮助企业获得新的增长点； 目前AI+游戏模式仍存在技术挑战，主要是3D模型数据存在大量缺失，在外部数据训练中，小体量数据难以起到显著作用，因此大模型大厂正持续整合3D数据资产。 具体内容 ","permalink":"https://vickycyq.github.io/zh/posts/blog/aigc/aigc+%E6%B8%B8%E6%88%8F%E5%9C%A8%E9%85%92%E6%B0%B4%E9%A5%AE%E6%96%99%E8%A1%8C%E4%B8%9A%E4%B8%AD%E5%BA%94%E7%94%A8%E7%9A%84%E7%A0%94%E7%A9%B6/","summary":"结论 AIGC可在游戏生产端、游戏运营端以及玩家体验端全方位赋能游戏产业，帮助游戏企业实现降本增效，开发创新玩法，提高玩家在游戏过程中的各项体","title":"AIGC+游戏在酒行业营销中的应用"},{"content":"结论 战略决策分析框架建立方法论 1. 战略决策规划推导 1.1 战略决策业务框架及业务闭环思路 通过拆解战略决策全生命旅程，结合华为“五看三定”模型以及DSTE框架，构建战略决策业务分析框架 战略决策全生命旅程包括： 战略规划 市场洞察 战略分析与规划 经营执行与评估 经营目标 经营策略 经营计划 经营预算 绩效考核 五看三定流程包括： 战略洞察 通过五看的方式（看宏观、看行业/趋势、看市场/客户、看竞争、看自己），分析消费趋势，寻找与竞争对手的差距点以及行业机会点。（注：华为原五看为看行业/趋势、看市场/客户、看竞争、看自己、看机会，根据实际情况修改） 战略分析与规划 通过“三定”的方式（定控制点、定目标、定策略）制定企业战略规划； 战略控制点：指公司核心竞争力所在，这种竞争优势很难轻易形成，竞争对手也难以轻易赶上。比如成本优势、性能和品质的领先、绝对的市场份额、专利技术等； 定目标：达标目标（自上而下）+挑战目标（自下而上）； 定策略：制定具体方案和阶段里程碑、实施策略 战略分解 战略分解也称为战略解码 战略执行监控与评估 DSTE框架包括三个二级流程： 战略规划流程（SP，Strategy Plan） 年度业务规划（BP，Business Plan） 战略执行与监控 1.2 战略决策业务指标拆解框架 1.3 战略决策主题分析框架 2. 分析子主题规划 2.1 宏观环境 2.1.1 经济分析 2.1.2 消费分析 2.1.3 收入分析 2.1.4 人口分析 2.2 市场分析 2.2.1 市场容量 2.2.2 市场结构 2.3 外部竞争 2.3.1 企业竞争态势 2.3.2 区域竞争态势 2.4 战略目标达成分析 2.4.1 产品 2.4.2 市场 2.4.3 渠道 2.4.4 消费者 2.4.5 组织与人才配置 ","permalink":"https://vickycyq.github.io/zh/posts/blog/%E9%A1%B9%E7%9B%AE%E5%A4%8D%E7%9B%98%E4%B8%8E%E6%80%9D%E8%80%83/%E6%88%98%E7%95%A5/","summary":"结论 战略决策分析框架建立方法论 1. 战略决策规划推导 1.1 战略决策业务框架及业务闭环思路 通过拆解战略决策全生命旅程，结合华为“五看三定”模型以及DS","title":"战略决策分析框架建立与思考"},{"content":"","permalink":"https://vickycyq.github.io/zh/link/","summary":"","title":"🔗友链"}]